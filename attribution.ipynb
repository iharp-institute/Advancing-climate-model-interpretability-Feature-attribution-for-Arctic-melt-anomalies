{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load Pretrained VAE Model\n",
    "# -------------------------------\n",
    "def sampling(args):\n",
    "    \"\"\"Sampling function for the VAE latent space.\"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "custom_objects = {'sampling': sampling}\n",
    "auto = load_model('/home/model-6', custom_objects=custom_objects)\n",
    "\n",
    "\n",
    "# Utility Functions\n",
    "# -------------------------------\n",
    "def print_clusters(df, cluster_labels):\n",
    "    \n",
    "    clusters = {}\n",
    "    for idx, lbl in enumerate(cluster_labels):\n",
    "        clusters.setdefault(lbl, []).append(df.columns[idx])\n",
    "    for lbl, feats in clusters.items():\n",
    "        print(f\"Cluster {lbl}: {', '.join(feats)}\")\n",
    "\n",
    "\n",
    "def create_clustered_dataframes(df, cluster_labels):\n",
    "    \n",
    "    clustered = {}\n",
    "    for lbl in np.unique(cluster_labels):\n",
    "        cols = [col for col, lab in zip(df.columns, cluster_labels) if lab == lbl]\n",
    "        clustered[lbl] = df[cols]\n",
    "    return clustered\n",
    "\n",
    "\n",
    "def create_sequences(clustered_dfs, seq_length):\n",
    "    \n",
    "    sequences = {}\n",
    "    for lbl, df in clustered_dfs.items():\n",
    "        arr = df.values  # shape: (time_steps, n_features_cluster)\n",
    "        xs = []\n",
    "        for i in range(len(arr) - seq_length):\n",
    "            xs.append(arr[i : i + seq_length])\n",
    "        sequences[lbl] = np.stack(xs)  # → shape (n_samples, seq_length, n_features_cluster)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def sequences_to_original(X_seq, seq_length):\n",
    "    \n",
    "    n_samples, L, n_feats = X_seq.shape\n",
    "    recon = np.zeros((n_samples + seq_length, n_feats))\n",
    "    for i in range(n_samples):\n",
    "        recon[i] = X_seq[i, 0]\n",
    "    recon[n_samples:] = X_seq[-1, :, :]\n",
    "    return recon\n",
    "\n",
    "\n",
    "# Prepare Data and Clustering\n",
    "# -------------------------------\n",
    "# NOTE: `train` and `test` must already be loaded pandas DataFrames with matching columns.\n",
    "\n",
    "# Ensure feature names are strings\n",
    "train.columns = train.columns.astype(str)\n",
    "test.columns  = test.columns.astype(str)\n",
    "\n",
    "# Determine optimal number of clusters\n",
    "n_clusters = optimal_cluster  # e.g., 4\n",
    "\n",
    "# Compute correlation matrix on training set\n",
    "corr_mat = train.corr()\n",
    "\n",
    "# Cluster features based on correlation\n",
    "clustering = KMeans(n_clusters=n_clusters, random_state=3).fit(corr_mat)\n",
    "cluster_labels = clustering.labels_\n",
    "\n",
    "print(\"Feature clusters (training data):\")\n",
    "print_clusters(train, cluster_labels)\n",
    "\n",
    "# Create Clustered DataFrames & Sequences\n",
    "# -------------------------------\n",
    "clustered_train_dfs = create_clustered_dataframes(train, cluster_labels)\n",
    "clustered_test_dfs  = create_clustered_dataframes(test, cluster_labels)\n",
    "\n",
    "# Sequence length\n",
    "T = 14\n",
    "np.random.seed(60)\n",
    "\n",
    "# Build rolling sequences for each cluster\n",
    "train_sequences = create_sequences(clustered_train_dfs, T)\n",
    "test_sequences  = create_sequences(clustered_test_dfs, T)\n",
    "\n",
    "\n",
    "# Feature Scoring \n",
    "# -------------------------------\n",
    "# Initialize DataFrame to hold mean‐absolute‐deviation scores per sample, per feature\n",
    "feature_scores = pd.DataFrame(index=test.index[T:], columns=test.columns)\n",
    "\n",
    "for feature in test.columns:\n",
    "    \n",
    "    test_variant = test.copy()\n",
    "    median_value = test[feature].median()\n",
    "    test_variant[feature] = median_value\n",
    "\n",
    "    clustered_variant = create_clustered_dataframes(test_variant, cluster_labels)\n",
    "    variant_sequences = create_sequences(clustered_variant, T)\n",
    "\n",
    "    input_list = [variant_sequences[lbl] for lbl in sorted(variant_sequences.keys())]\n",
    "    pred_seq = auto.predict(input_list, verbose=0)\n",
    "    pred_recon = sequences_to_original(pred_seq, T)\n",
    "\n",
    "    actual_concat = pd.concat(\n",
    "        [clustered_variant[lbl] for lbl in sorted(clustered_variant.keys())],\n",
    "        axis=1\n",
    "    ).values\n",
    "\n",
    "    n_samples = pred_seq.shape[0]\n",
    "    mad = np.mean(np.abs(pred_recon[:n_samples] - actual_concat[:n_samples]), axis=1)\n",
    "\n",
    "    feature_scores[feature] = mad\n",
    "\n",
    "    print(f\"Feature: {feature}, mean MAD (over all samples): {mad.mean():.6f}\")\n",
    "\n",
    "\n",
    "# Identify Top “Most Contributive” Feature per Time Step\n",
    "# -------------------------------\n",
    "# We compare each feature’s MAD to the ‘Loss_mae’ column; features exceeding that are candidates\n",
    "max_value_cols = []\n",
    "for idx, row in feature_scores.iterrows():\n",
    "    main_val = row['Loss_mae']\n",
    "    # Filter features whose score > main_val\n",
    "    higher = {col: row[col] for col in feature_scores.columns if col != 'Loss_mae' and row[col] > main_val}\n",
    "    max_col = max(higher, key=higher.get) if higher else None\n",
    "    max_value_cols.append(max_col)\n",
    "\n",
    "feature_scores['max_value_column'] = max_value_cols\n",
    "\n",
    "# Save and Plot Results\n",
    "# -------------------------------\n",
    "feature_scores.to_csv('/home/feature_score-6.csv')\n",
    "\n",
    "# Plot the top 20 features by appearance in max_value_column\n",
    "value_counts = feature_scores['max_value_column'].value_counts().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(\n",
    "    value_counts[:20].index[::-1],\n",
    "    value_counts[:20].values[::-1]\n",
    ")\n",
    "plt.xlabel(\"Counts\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Top 20 Features Appearing in 'max_value_column'\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
