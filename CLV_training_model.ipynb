{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebeeb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, RepeatVector, TimeDistributed, Dense,\n",
    "    Concatenate, Lambda, Dropout\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "# Compute correlation matrix on the training set\n",
    "corr_mat = train_scaled.corr()\n",
    "\n",
    "# 3a. Elbow Method: compute SSE for cluster counts 1–9\n",
    "sse = []\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=2000, random_state=42)\n",
    "    kmeans.fit(corr_mat)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "frame_elbow = pd.DataFrame({'Cluster': range(1, 10), 'SSE': sse})\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(frame_elbow['Cluster'], frame_elbow['SSE'], marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('SSE (Inertia)')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Silhouette Method: evaluate silhouette score for k = 2–6\n",
    "silhouette_scores = {}\n",
    "for k in range(2, 7):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(corr_mat)\n",
    "    score = silhouette_score(corr_mat, labels)\n",
    "    silhouette_scores[k] = score\n",
    "    print(f\"Silhouette Score for {k} clusters: {score:.4f}\")\n",
    "\n",
    "# Choose the k with the highest silhouette score\n",
    "optimal_clusters = max(silhouette_scores, key=silhouette_scores.get)\n",
    "print(f\"\\nOptimal number of clusters based on silhouette score: {optimal_clusters}\")\n",
    "\n",
    "# Cluster Features on the Correlation Matrix\n",
    "# -------------------------------\n",
    "kmeans_final = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "cluster_labels = kmeans_final.fit_predict(corr_mat)\n",
    "\n",
    "\n",
    "# Utility Functions\n",
    "# ------------------------------------------\n",
    "def print_clusters(df, cluster_labels):\n",
    "    \"\"\"\n",
    "    Print feature names belonging to each cluster.\n",
    "    \"\"\"\n",
    "    clusters = {}\n",
    "    for idx, lbl in enumerate(cluster_labels):\n",
    "        clusters.setdefault(lbl, []).append(df.columns[idx])\n",
    "    for lbl, feats in clusters.items():\n",
    "        print(f\"Cluster {lbl}: {', '.join(feats)}\")\n",
    "\n",
    "\n",
    "def create_clustered_dataframes(df, cluster_labels):\n",
    "    \"\"\"\n",
    "    Given a DataFrame `df` and an array of cluster labels (one label per column),\n",
    "    return a dict mapping each label → DataFrame of the columns in that cluster.\n",
    "    \"\"\"\n",
    "    clustered = {}\n",
    "    for lbl in np.unique(cluster_labels):\n",
    "        cols = [col for col, lab in zip(df.columns, cluster_labels) if lab == lbl]\n",
    "        clustered[lbl] = df[cols]\n",
    "    return clustered\n",
    "\n",
    "\n",
    "def create_sequences(clustered_dfs, seq_length):\n",
    "    \"\"\"\n",
    "    For each clustered DataFrame in `clustered_dfs`, build a 3D array of rolling\n",
    "    sequences of length `seq_length`. Returns {cluster_label: np.array(shape=(n_samples, seq_length, n_features))}.\n",
    "    \"\"\"\n",
    "    sequences = {}\n",
    "    for lbl, df in clustered_dfs.items():\n",
    "        arr = df.values  # shape: (time_steps, n_features_cluster)\n",
    "        xs = []\n",
    "        for i in range(len(arr) - seq_length):\n",
    "            xs.append(arr[i : i + seq_length])\n",
    "        sequences[lbl] = np.stack(xs)  # → shape (n_samples, seq_length, n_features_cluster)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def sequences_to_original(X_seq, seq_length):\n",
    "    \"\"\"\n",
    "    Reconstruct a time series from the VAE output sequences.\n",
    "    X_seq: array of shape (n_samples, seq_length, n_features)\n",
    "    Returns an array of shape (n_samples + seq_length, n_features) where each\n",
    "    time step i is taken from X_seq[i, 0, :], and the final `seq_length` steps\n",
    "    come from X_seq[-1, :, :].\n",
    "    \"\"\"\n",
    "    n_samples, L, n_feats = X_seq.shape\n",
    "    recon = np.zeros((n_samples + seq_length, n_feats))\n",
    "    for i in range(n_samples):\n",
    "        recon[i] = X_seq[i, 0]\n",
    "    recon[n_samples:] = X_seq[-1, :, :]\n",
    "    return recon\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"\n",
    "    Sampling layer for VAE: z = z_mean + exp(0.5 * z_log_var) * epsilon\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "# VAE Model Definition\n",
    "# ------------------------------------------\n",
    "def vae_model(input_seq_list, kl_weight=0.75, latent_dim=8, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Build a VAE that takes a list of input sequences (one per cluster).\n",
    "    Each branch encodes its cluster-specific sequence into a latent vector,\n",
    "    and then we concatenate all latent vectors and decode back to the full feature set.\n",
    "\n",
    "    input_seq_list: list of np.arrays, each of shape (n_samples, seq_length, n_feats_cluster)\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    latents = []\n",
    "    kl_losses = []\n",
    "\n",
    "    # Determine total number of features across all clusters\n",
    "    total_features = sum([X.shape[2] for X in input_seq_list])\n",
    "    seq_len = input_seq_list[0].shape[1]\n",
    "\n",
    "    # Build one encoder branch per cluster\n",
    "    for X in input_seq_list:\n",
    "        inp = Input(shape=(seq_len, X.shape[2]))\n",
    "        h = LSTM(64, activation='relu', return_sequences=True)(inp)\n",
    "        h = Dropout(dropout_rate)(h)\n",
    "        h = LSTM(32, activation='relu', return_sequences=True)(h)\n",
    "        h = Dropout(dropout_rate)(h)\n",
    "        h = LSTM(16, activation='relu', return_sequences=True)(h)\n",
    "        h = LSTM(8, activation='relu', return_sequences=False)(h)\n",
    "\n",
    "        z_mean = Dense(latent_dim, kernel_regularizer=regularizers.l2(1e-2))(h)\n",
    "        z_log_var = Dense(latent_dim, kernel_regularizer=regularizers.l2(1e-2))(h)\n",
    "        z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "        inputs.append(inp)\n",
    "        latents.append(z)\n",
    "\n",
    "        # KL divergence for this branch\n",
    "        kl = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        kl_losses.append(kl)\n",
    "\n",
    "    # Concatenate latent vectors from all clusters\n",
    "    merged = Concatenate(axis=-1)(latents)\n",
    "    \n",
    "    dec = RepeatVector(seq_len)(merged)\n",
    "    dec = LSTM(8, activation='relu', return_sequences=True)(dec)\n",
    "    dec = LSTM(16, activation='relu', return_sequences=True)(dec)\n",
    "    dec = LSTM(32, activation='relu', return_sequences=True)(dec)\n",
    "    dec = LSTM(64, activation='relu', return_sequences=True)(dec)\n",
    "\n",
    "    output = TimeDistributed(Dense(total_features))(dec)\n",
    "\n",
    "    vae = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    # Add KL loss (sum over all branches)\n",
    "    total_kl = K.mean(tf.add_n(kl_losses))\n",
    "    vae.add_loss(kl_weight * total_kl)\n",
    "\n",
    "    vae.compile(optimizer=Adam(learning_rate=1e-4), loss='mse')\n",
    "    return vae\n",
    "\n",
    "\n",
    "\n",
    "# Main Pipeline (Training & Testing)\n",
    "# ------------------------------------------\n",
    "\n",
    "# Convert column names to strings (required for concatenation later)\n",
    "train.columns = train.columns.astype(str)\n",
    "test.columns  = test.columns.astype(str)\n",
    "\n",
    "# Cluster the features based on correlation\n",
    "n_clusters = optimal_clusters\n",
    "corr_mat = train.corr()\n",
    "clustering = KMeans(n_clusters=n_clusters, random_state=0).fit(corr_mat)\n",
    "cluster_labels = clustering.labels_\n",
    "\n",
    "print(\"Feature clusters (training data):\")\n",
    "print_clusters(train, cluster_labels)\n",
    "\n",
    "# Split `train` and `test` into cluster‐specific DataFrames\n",
    "clustered_train_dfs = create_clustered_dataframes(train, cluster_labels)\n",
    "clustered_test_dfs  = create_clustered_dataframes(test, cluster_labels)\n",
    "\n",
    "# Sequence length\n",
    "T = 14\n",
    "np.random.seed(40)\n",
    "\n",
    "# Build rolling sequences for each cluster\n",
    "train_sequences = create_sequences(clustered_train_dfs, T)\n",
    "test_sequences  = create_sequences(clustered_test_dfs, T)\n",
    "\n",
    "# Prepare model inputs as a list of arrays, one per cluster\n",
    "train_input_list = [train_sequences[lbl] for lbl in sorted(train_sequences.keys())]\n",
    "test_input_list  = [test_sequences[lbl]  for lbl in sorted(test_sequences.keys())]\n",
    "\n",
    "vae = vae_model(train_input_list, kl_weight=0.75, latent_dim=8, dropout_rate=0.2)\n",
    "\n",
    "# We need to construct the “true” output by concatenating all cluster inputs along the feature axis:\n",
    "true_train = np.concatenate(train_input_list, axis=-1)  # shape: (n_samples, T, total_features)\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=1e-3,\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = vae.fit(\n",
    "    train_input_list,\n",
    "    true_train,         \n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# \n",
    "train_pred_seq = vae.predict(train_input_list)\n",
    "train_pred_recon = sequences_to_original(train_pred_seq, T)\n",
    "\n",
    "# Reconstruct actual training data in the same concatenated order\n",
    "actual_train = pd.concat(\n",
    "    [clustered_train_dfs[lbl] for lbl in sorted(clustered_train_dfs.keys())],\n",
    "    axis=1\n",
    ").values\n",
    "\n",
    "n_train_samples = train_pred_seq.shape[0]\n",
    "train_mse = mean_squared_error(\n",
    "    actual_train[:n_train_samples], train_pred_recon[:n_train_samples]\n",
    ")\n",
    "print(f\"Train Mean Squared Error: {train_mse:.6f}\")\n",
    "\n",
    "# 3h. Evaluate on Test Set\n",
    "test_pred_seq = vae.predict(test_input_list, batch_size=256)\n",
    "test_pred_recon = sequences_to_original(test_pred_seq, T)\n",
    "\n",
    "actual_test = pd.concat(\n",
    "    [clustered_test_dfs[lbl] for lbl in sorted(clustered_test_dfs.keys())],\n",
    "    axis=1\n",
    ").values\n",
    "\n",
    "n_test_samples = test_pred_seq.shape[0]\n",
    "test_mse = mean_squared_error(\n",
    "    actual_test[:n_test_samples], test_pred_recon[:n_test_samples]\n",
    ")\n",
    "print(f\"Test Mean Squared Error: {test_mse:.6f}\")\n",
    "vae.save('/home/smlt_model')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
